#!/usr/bin/env python3
"""
Whisper ëª¨ë¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë””ë²„ê¹…
"""

import torch
import whisper
import gc
import os

def debug_memory_step(step_name):
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3
        cached = torch.cuda.memory_reserved() / 1024**3
        total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"{step_name}: í• ë‹¹={allocated:.2f}GB, ìºì‹œ={cached:.2f}GB, ì´ì‚¬ìš©={allocated+cached:.2f}GB")
        return allocated + cached
    return 0

def debug_whisper_loading():
    """Whisper ëª¨ë¸ ë¡œë”© ê³¼ì • ë””ë²„ê¹…"""
    print("=== Whisper ëª¨ë¸ ë¡œë”© ë©”ëª¨ë¦¬ ë””ë²„ê¹… ===")
    
    if not torch.cuda.is_available():
        print("âŒ CUDA ì‚¬ìš© ë¶ˆê°€")
        return
    
    # ì´ˆê¸° ìƒíƒœ
    torch.cuda.empty_cache()
    gc.collect()
    debug_memory_step("1. ì´ˆê¸° ìƒíƒœ")
    
    # PyTorch ì„¤ì •
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:256'
    torch.cuda.set_per_process_memory_fraction(0.98)
    debug_memory_step("2. PyTorch ì„¤ì • í›„")
    
    # ê° ëª¨ë¸ë³„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
    models_to_test = ["tiny", "base", "small", "medium"]
    
    for model_name in models_to_test:
        print(f"\n--- {model_name} ëª¨ë¸ í…ŒìŠ¤íŠ¸ ---")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        torch.cuda.empty_cache()
        gc.collect()
        before_memory = debug_memory_step(f"{model_name} ë¡œë”© ì „")
        
        try:
            # ëª¨ë¸ ë¡œë”©
            print(f"{model_name} ëª¨ë¸ ë¡œë”© ì¤‘...")
            model = whisper.load_model(model_name, device="cuda")
            after_memory = debug_memory_step(f"{model_name} ë¡œë”© í›„")
            
            # ì‹¤ì œ ì‚¬ìš©ëŸ‰ ê³„ì‚°
            used_memory = after_memory - before_memory
            print(f"âœ… {model_name} ëª¨ë¸ ì‹¤ì œ ì‚¬ìš©ëŸ‰: {used_memory:.2f}GB")
            
            # ëª¨ë¸ ì •ë¦¬
            del model
            torch.cuda.empty_cache()
            gc.collect()
            debug_memory_step(f"{model_name} ì •ë¦¬ í›„")
            
        except RuntimeError as e:
            if "CUDA out of memory" in str(e):
                print(f"âŒ {model_name} ëª¨ë¸ ë©”ëª¨ë¦¬ ë¶€ì¡±: {e}")
                break
            else:
                print(f"âŒ {model_name} ëª¨ë¸ ì˜¤ë¥˜: {e}")
    
    # Large ëª¨ë¸ ì‹œë„ (ì‹¤íŒ¨ ì˜ˆìƒ)
    print(f"\n--- large-v2 ëª¨ë¸ í…ŒìŠ¤íŠ¸ ---")
    torch.cuda.empty_cache()
    gc.collect()
    before_memory = debug_memory_step("large-v2 ë¡œë”© ì „")
    
    try:
        print("large-v2 ëª¨ë¸ ë¡œë”© ì‹œë„...")
        model = whisper.load_model("large-v2", device="cuda")
        after_memory = debug_memory_step("large-v2 ë¡œë”© í›„")
        used_memory = after_memory - before_memory
        print(f"ğŸ† large-v2 ëª¨ë¸ ì‹¤ì œ ì‚¬ìš©ëŸ‰: {used_memory:.2f}GB")
        del model
    except RuntimeError as e:
        if "CUDA out of memory" in str(e):
            print(f"âŒ large-v2 ëª¨ë¸ ë©”ëª¨ë¦¬ ë¶€ì¡±")
            # ì˜¤ë¥˜ ë©”ì‹œì§€ì—ì„œ ì‹¤ì œ í•„ìš” ë©”ëª¨ë¦¬ ì¶”ì¶œ
            import re
            match = re.search(r'Tried to allocate (\d+\.\d+) (\w+)', str(e))
            if match:
                size = float(match.group(1))
                unit = match.group(2)
                if unit == "MiB":
                    size_gb = size / 1024
                elif unit == "GiB":
                    size_gb = size
                print(f"ì¶”ê°€ í•„ìš” ë©”ëª¨ë¦¬: {size_gb:.2f}GB")
                print(f"ì˜ˆìƒ ì´ í•„ìš”ëŸ‰: {before_memory + size_gb:.2f}GB")
        else:
            print(f"âŒ large-v2 ëª¨ë¸ ê¸°íƒ€ ì˜¤ë¥˜: {e}")

def check_whisper_model_sizes():
    """Whisper ëª¨ë¸ ê³µì‹ í¬ê¸° í™•ì¸"""
    print("\n=== Whisper ëª¨ë¸ ê³µì‹ í¬ê¸° ===")
    
    # Whisper ëª¨ë¸ ì •ë³´ (ê³µì‹)
    model_info = {
        "tiny": "39 MB",
        "base": "74 MB", 
        "small": "244 MB",
        "medium": "769 MB",
        "large": "1550 MB",
        "large-v2": "1550 MB",
        "large-v3": "1550 MB"
    }
    
    for model, size in model_info.items():
        print(f"{model:10}: {size}")
    
    print("\nâ“ ê·¸ëŸ°ë° ì™œ GPUì—ì„œëŠ” ë” ë§ì€ ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í• ê¹Œ?")
    print("1. ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë”©")
    print("2. GPU ë©”ëª¨ë¦¬ ì •ë ¬ ë° íŒ¨ë”©")
    print("3. ì¤‘ê°„ ê³„ì‚° ë²„í¼")
    print("4. PyTorch ì˜¤ë²„í—¤ë“œ")

if __name__ == "__main__":
    check_whisper_model_sizes()
    debug_whisper_loading() 