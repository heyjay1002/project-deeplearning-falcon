#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TCN ëª¨ë¸ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
í•™ìŠµëœ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
import koreanize_matplotlib
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.metrics import precision_recall_fscore_support, roc_curve, auc
from sklearn.metrics import precision_recall_curve, f1_score
from sklearn.preprocessing import label_binarize
import logging
from pathlib import Path
import json
from datetime import datetime
import pandas as pd
from collections import Counter

from config import TCN_CONFIG, GESTURE_CLASSES, PATHS, TRAINING_CONFIG
from model import TCNGestureClassifier
from dataset import GestureDataset

# Create English class names list in order
GESTURE_CLASSES_EN = [GESTURE_CLASSES[i] for i in range(len(GESTURE_CLASSES))]
from torch.utils.data import DataLoader

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
sns.set_style("whitegrid")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ModelEvaluator:
    """TCN ëª¨ë¸ ì¢…í•© í‰ê°€ í´ë˜ìŠ¤"""
    
    def __init__(self, model_path: str = None):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model_path = model_path or PATHS['model_file']
        self.model = None
        self.test_loader = None
        
        # ê²°ê³¼ ì €ì¥ í´ë”
        self.results_dir = Path("evaluation_results")
        self.results_dir.mkdir(exist_ok=True)
        
    def load_model(self):
        """í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ"""
        try:
            self.model = TCNGestureClassifier(
                input_size=TCN_CONFIG['input_size'],
                num_channels=TCN_CONFIG['num_channels'],
                kernel_size=TCN_CONFIG['kernel_size'],
                dropout=TCN_CONFIG['dropout'],
                num_classes=TCN_CONFIG['num_classes']
            )
            
            if Path(self.model_path).exists():
                checkpoint = torch.load(self.model_path, map_location=self.device)
                if 'model_state_dict' in checkpoint:
                    # checkpoint í˜•íƒœë¡œ ì €ì¥ëœ ê²½ìš°
                    self.model.load_state_dict(checkpoint['model_state_dict'])
                else:
                    # state_dictë§Œ ì €ì¥ëœ ê²½ìš°
                    self.model.load_state_dict(checkpoint)
                
                self.model.to(self.device)
                self.model.eval()
                logger.info(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.model_path}")
                return True
            else:
                logger.error(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.model_path}")
                return False
                
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
            return False
    
    def load_test_data(self):
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ"""
        try:
            test_dataset = GestureDataset(
                data_root=PATHS['processed_data'],
                split='test',
                test_size=1.0 - TRAINING_CONFIG['train_test_split']
            )
            
            self.test_loader = DataLoader(
                test_dataset,
                batch_size=TRAINING_CONFIG['batch_size'],
                shuffle=False,
                num_workers=2
            )
            
            logger.info(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(test_dataset)}ê°œ ìƒ˜í”Œ")
            return True
            
        except Exception as e:
            logger.error(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
            return False
    
    def evaluate_model(self):
        """ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""
        if not self.model or not self.test_loader:
            logger.error("ëª¨ë¸ ë˜ëŠ” ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return None
        
        all_predictions = []
        all_targets = []
        all_confidences = []
        
        with torch.no_grad():
            for batch_data, batch_targets in self.test_loader:
                batch_data = batch_data.to(self.device)
                batch_targets = batch_targets.to(self.device)
                
                # ì˜ˆì¸¡
                outputs = self.model(batch_data)
                probabilities = torch.softmax(outputs, dim=1)
                predictions = torch.argmax(outputs, dim=1)
                
                # ê²°ê³¼ ì €ì¥
                all_predictions.extend(predictions.cpu().numpy())
                all_targets.extend(batch_targets.cpu().numpy())
                all_confidences.extend(torch.max(probabilities, dim=1)[0].cpu().numpy())
        
        return {
            'predictions': np.array(all_predictions),
            'targets': np.array(all_targets),
            'confidences': np.array(all_confidences)
        }
    
    def calculate_metrics(self, results):
        """ìƒì„¸ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°"""
        predictions = results['predictions']
        targets = results['targets']
        confidences = results['confidences']
        
        # ê¸°ë³¸ ì§€í‘œ
        accuracy = accuracy_score(targets, predictions)
        precision, recall, f1, _ = precision_recall_fscore_support(
            targets, predictions, average='weighted'
        )
        
        # í´ë˜ìŠ¤ë³„ ì§€í‘œ
        class_report = classification_report(
            targets, predictions,
            target_names=GESTURE_CLASSES_EN,
            output_dict=True
        )
        
        # ì‹ ë¢°ë„ í†µê³„ (JSON ì§ë ¬í™”ë¥¼ ìœ„í•´ float ë³€í™˜)
        confidence_stats = {
            'mean': float(np.mean(confidences)),
            'std': float(np.std(confidences)),
            'min': float(np.min(confidences)),
            'max': float(np.max(confidences))
        }
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'class_report': class_report,
            'confidence_stats': confidence_stats
        }
    
    def plot_confusion_matrix(self, results, save_path=None):
        """Confusion Matrix ì‹œê°í™”"""
        predictions = results['predictions']
        targets = results['targets']
        
        # Confusion Matrix ê³„ì‚°
        cm = confusion_matrix(targets, predictions)
        
        # ì‹œê°í™”
        plt.figure(figsize=(10, 8))
        sns.heatmap(
            cm, 
            annot=True, 
            fmt='d',
            cmap='Blues',
            xticklabels=GESTURE_CLASSES_EN,
            yticklabels=GESTURE_CLASSES_EN
        )
        plt.title('Gesture Recognition Confusion Matrix', fontsize=14, fontweight='bold')
        plt.xlabel('Predicted Gesture', fontsize=12)
        plt.ylabel('Actual Gesture', fontsize=12)
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"Confusion Matrix saved: {save_path}")
        
        plt.show()
    
    def plot_confidence_distribution(self, results, save_path=None):
        """ì‹ ë¢°ë„ ë¶„í¬ ì‹œê°í™”"""
        confidences = results['confidences']
        predictions = results['predictions']
        
        plt.figure(figsize=(12, 8))
        
        # ì „ì²´ ì‹ ë¢°ë„ íˆìŠ¤í† ê·¸ë¨
        plt.subplot(2, 2, 1)
        plt.hist(confidences, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
        plt.title('Overall Confidence Distribution')
        plt.xlabel('Confidence')
        plt.ylabel('Frequency')
        
        # í´ë˜ìŠ¤ë³„ ì‹ ë¢°ë„ ë°•ìŠ¤í”Œë¡¯
        plt.subplot(2, 2, 2)
        confidence_by_class = [confidences[predictions == i] for i in range(len(GESTURE_CLASSES))]
        plt.boxplot(confidence_by_class, labels=GESTURE_CLASSES_EN)
        plt.title('Confidence Distribution by Class')
        plt.ylabel('Confidence')
        plt.xticks(rotation=45)
        
        # ì‹ ë¢°ë„ vs ì •í™•ë„
        plt.subplot(2, 2, 3)
        correct = (predictions == results['targets'])
        plt.scatter(confidences[correct], [1]*sum(correct), alpha=0.5, label='Correct', color='green')
        plt.scatter(confidences[~correct], [0]*sum(~correct), alpha=0.5, label='Incorrect', color='red')
        plt.title('Confidence vs Correctness')
        plt.xlabel('Confidence')
        plt.ylabel('Correct/Incorrect')
        plt.legend()
        
        # ì‹ ë¢°ë„ ì„ê³„ê°’ë³„ ì •í™•ë„
        plt.subplot(2, 2, 4)
        thresholds = np.arange(0.5, 1.0, 0.05)
        accuracies = []
        for threshold in thresholds:
            mask = confidences >= threshold
            if sum(mask) > 0:
                acc = accuracy_score(results['targets'][mask], predictions[mask])
                accuracies.append(acc)
            else:
                accuracies.append(0)
        
        plt.plot(thresholds, accuracies, marker='o')
        plt.title('Accuracy by Confidence Threshold')
        plt.xlabel('Confidence Threshold')
        plt.ylabel('Accuracy')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"Confidence distribution saved: {save_path}")
        
        plt.show()
    
    def plot_roc_curves(self, results, save_path=None):
        """ROC ê³¡ì„  ì‹œê°í™” (Multiclass)"""
        predictions = results['predictions']
        targets = results['targets']
        
        # One-hot encode targets
        y_true = label_binarize(targets, classes=list(range(len(GESTURE_CLASSES))))
        y_pred = label_binarize(predictions, classes=list(range(len(GESTURE_CLASSES))))
        
        plt.figure(figsize=(12, 8))
        
        colors = ['blue', 'red', 'green', 'orange', 'purple']
        
        for i in range(len(GESTURE_CLASSES)):
            if i < len(colors):
                color = colors[i]
            else:
                color = np.random.rand(3,)
                
            # Calculate ROC curve
            fpr, tpr, _ = roc_curve(y_true[:, i], y_pred[:, i])
            roc_auc = auc(fpr, tpr)
            
            plt.plot(fpr, tpr, color=color, lw=2,
                    label=f'{GESTURE_CLASSES_EN[i]} (AUC = {roc_auc:.2f})')
        
        plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('ROC Curves for Each Gesture Class')
        plt.legend(loc="lower right")
        plt.grid(True, alpha=0.3)
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"ROC curves saved: {save_path}")
        
        plt.show()
    
    def plot_class_performance_comparison(self, results, save_path=None):
        """í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¹„êµ ë°” ì°¨íŠ¸"""
        predictions = results['predictions']
        targets = results['targets']
        
        # í´ë˜ìŠ¤ë³„ ì§€í‘œ ê³„ì‚°
        class_metrics = {}
        for i, class_name in enumerate(GESTURE_CLASSES_EN):
            mask = targets == i
            if np.sum(mask) > 0:
                class_preds = predictions[mask]
                class_accuracy = np.mean(class_preds == i)
                
                # ì „ì²´ì—ì„œ ì´ í´ë˜ìŠ¤ì— ëŒ€í•œ precision, recall ê³„ì‚°
                tp = np.sum((predictions == i) & (targets == i))
                fp = np.sum((predictions == i) & (targets != i))
                fn = np.sum((predictions != i) & (targets == i))
                
                precision = tp / (tp + fp) if (tp + fp) > 0 else 0
                recall = tp / (tp + fn) if (tp + fn) > 0 else 0
                f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
                
                class_metrics[class_name] = {
                    'accuracy': class_accuracy,
                    'precision': precision,
                    'recall': recall,
                    'f1_score': f1
                }
        
        # ì‹œê°í™”
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Class-wise Performance Comparison', fontsize=16, fontweight='bold')
        
        metrics_names = ['accuracy', 'precision', 'recall', 'f1_score']
        metric_titles = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
        
        for idx, (metric, title) in enumerate(zip(metrics_names, metric_titles)):
            ax = axes[idx // 2, idx % 2]
            
            classes = list(class_metrics.keys())
            values = [class_metrics[cls][metric] for cls in classes]
            
            bars = ax.bar(classes, values, color=['skyblue', 'lightcoral', 'lightgreen', 'orange'])
            ax.set_title(f'{title} by Class')
            ax.set_ylabel(title)
            ax.set_ylim(0, 1.1)
            
            # ê°’ í‘œì‹œ
            for bar, value in zip(bars, values):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                       f'{value:.3f}', ha='center', va='bottom')
            
            ax.tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"Class performance comparison saved: {save_path}")
        
        plt.show()
    
    def plot_error_analysis(self, results, save_path=None):
        """ì˜¤ë¶„ë¥˜ ë¶„ì„ ì‹œê°í™”"""
        predictions = results['predictions']
        targets = results['targets']
        confidences = results['confidences']
        
        # ì˜¤ë¶„ë¥˜ ì°¾ê¸°
        misclassified = predictions != targets
        
        if np.sum(misclassified) == 0:
            print("No misclassifications found!")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Error Analysis', fontsize=16, fontweight='bold')
        
        # 1. ì˜¤ë¶„ë¥˜ ê°œìˆ˜ by ì‹¤ì œ í´ë˜ìŠ¤
        ax1 = axes[0, 0]
        error_by_true_class = []
        for i in range(len(GESTURE_CLASSES)):
            errors = np.sum(misclassified & (targets == i))
            error_by_true_class.append(errors)
        
        bars1 = ax1.bar(GESTURE_CLASSES_EN, error_by_true_class, color='lightcoral')
        ax1.set_title('Misclassifications by True Class')
        ax1.set_ylabel('Number of Errors')
        ax1.tick_params(axis='x', rotation=45)
        
        for bar, value in zip(bars1, error_by_true_class):
            if value > 0:
                ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.1,
                        str(value), ha='center', va='bottom')
        
        # 2. ì˜¤ë¶„ë¥˜ ê°œìˆ˜ by ì˜ˆì¸¡ í´ë˜ìŠ¤
        ax2 = axes[0, 1]
        error_by_pred_class = []
        for i in range(len(GESTURE_CLASSES)):
            errors = np.sum(misclassified & (predictions == i))
            error_by_pred_class.append(errors)
        
        bars2 = ax2.bar(GESTURE_CLASSES_EN, error_by_pred_class, color='lightblue')
        ax2.set_title('Misclassifications by Predicted Class')
        ax2.set_ylabel('Number of Errors')
        ax2.tick_params(axis='x', rotation=45)
        
        for bar, value in zip(bars2, error_by_pred_class):
            if value > 0:
                ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.1,
                        str(value), ha='center', va='bottom')
        
        # 3. ì˜¤ë¶„ë¥˜ì˜ ì‹ ë¢°ë„ ë¶„í¬
        ax3 = axes[1, 0]
        correct_confidences = confidences[~misclassified]
        incorrect_confidences = confidences[misclassified]
        
        ax3.hist(correct_confidences, bins=20, alpha=0.7, label='Correct', color='green', density=True)
        ax3.hist(incorrect_confidences, bins=20, alpha=0.7, label='Incorrect', color='red', density=True)
        ax3.set_title('Confidence Distribution: Correct vs Incorrect')
        ax3.set_xlabel('Confidence')
        ax3.set_ylabel('Density')
        ax3.legend()
        
        # 4. ì‹ ë¢°ë„ vs ì •í™•ë„ (êµ¬ê°„ë³„)
        ax4 = axes[1, 1]
        confidence_bins = np.arange(0.0, 1.01, 0.1)
        bin_accuracies = []
        bin_centers = []
        
        for i in range(len(confidence_bins) - 1):
            mask = (confidences >= confidence_bins[i]) & (confidences < confidence_bins[i+1])
            if np.sum(mask) > 0:
                bin_accuracy = np.mean(predictions[mask] == targets[mask])
                bin_accuracies.append(bin_accuracy)
                bin_centers.append((confidence_bins[i] + confidence_bins[i+1]) / 2)
        
        ax4.plot(bin_centers, bin_accuracies, 'bo-', linewidth=2, markersize=8)
        ax4.set_title('Accuracy vs Confidence (Binned)')
        ax4.set_xlabel('Confidence Bin Center')
        ax4.set_ylabel('Accuracy')
        ax4.grid(True, alpha=0.3)
        ax4.set_ylim(0, 1.1)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"Error analysis saved: {save_path}")
        
        plt.show()
    
    def plot_performance_summary(self, metrics, save_path=None):
        """ì„±ëŠ¥ ì§€í‘œ ì¢…í•© ìš”ì•½ ì°¨íŠ¸"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Model Performance Summary', fontsize=16, fontweight='bold')
        
        # 1. ì „ì²´ ì„±ëŠ¥ ì§€í‘œ ë ˆì´ë” ì°¨íŠ¸
        ax1 = axes[0, 0]
        
        overall_metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
        values = [
            metrics['accuracy'],
            metrics['precision'], 
            metrics['recall'],
            metrics['f1_score']
        ]
        
        # ë ˆì´ë” ì°¨íŠ¸ ëŒ€ì‹  ë°” ì°¨íŠ¸ ì‚¬ìš© (ë” ëª…í™•í•¨)
        bars = ax1.bar(overall_metrics, values, color=['gold', 'lightblue', 'lightgreen', 'coral'])
        ax1.set_title('Overall Performance Metrics')
        ax1.set_ylabel('Score')
        ax1.set_ylim(0, 1.1)
        
        for bar, value in zip(bars, values):
            ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,
                    f'{value:.3f}', ha='center', va='bottom', fontweight='bold')
        
        # 2. ì‹ ë¢°ë„ í†µê³„
        ax2 = axes[0, 1]
        conf_stats = metrics['confidence_stats']
        stats_names = ['Mean', 'Std', 'Min', 'Max']
        stats_values = [conf_stats['mean'], conf_stats['std'], conf_stats['min'], conf_stats['max']]
        
        bars2 = ax2.bar(stats_names, stats_values, color=['purple', 'orange', 'red', 'green'])
        ax2.set_title('Confidence Statistics')
        ax2.set_ylabel('Confidence Value')
        
        for bar, value in zip(bars2, stats_values):
            ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,
                    f'{value:.3f}', ha='center', va='bottom')
        
        # 3. í´ë˜ìŠ¤ë³„ ì§€ì› ìˆ˜ (Support)
        ax3 = axes[1, 0]
        class_report = metrics['class_report']
        
        supports = []
        for class_name in GESTURE_CLASSES_EN:
            if class_name in class_report:
                supports.append(class_report[class_name]['support'])
            else:
                supports.append(0)
        
        bars3 = ax3.bar(GESTURE_CLASSES_EN, supports, color='lightcyan')
        ax3.set_title('Test Samples per Class')
        ax3.set_ylabel('Number of Samples')
        ax3.tick_params(axis='x', rotation=45)
        
        for bar, value in zip(bars3, supports):
            ax3.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 1,
                    str(int(value)), ha='center', va='bottom')
        
        # 4. ì„±ëŠ¥ ë“±ê¸‰ í‘œì‹œ
        ax4 = axes[1, 1]
        
        # ì„±ëŠ¥ ë“±ê¸‰ ê²°ì •
        accuracy = metrics['accuracy']
        if accuracy >= 0.98:
            grade = 'Excellent'
            color = 'green'
        elif accuracy >= 0.95:
            grade = 'Very Good'
            color = 'lightgreen'
        elif accuracy >= 0.90:
            grade = 'Good'
            color = 'yellow'
        elif accuracy >= 0.80:
            grade = 'Fair'
            color = 'orange'
        else:
            grade = 'Poor'
            color = 'red'
        
        ax4.text(0.5, 0.7, f'Model Grade', ha='center', va='center', fontsize=16, fontweight='bold')
        ax4.text(0.5, 0.5, grade, ha='center', va='center', fontsize=24, fontweight='bold', color=color)
        ax4.text(0.5, 0.3, f'Accuracy: {accuracy:.1%}', ha='center', va='center', fontsize=14)
        ax4.set_xlim(0, 1)
        ax4.set_ylim(0, 1)
        ax4.axis('off')
        
        # ë°°ê²½ ìƒ‰ìƒ ì„¤ì •
        ax4.add_patch(plt.Rectangle((0.1, 0.1), 0.8, 0.8, facecolor=color, alpha=0.1))
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"Performance summary saved: {save_path}")
        
        plt.show()
    
    def analyze_misclassifications(self, results):
        """ì˜¤ë¶„ë¥˜ ìƒì„¸ ë¶„ì„"""
        predictions = results['predictions']
        targets = results['targets']
        confidences = results['confidences']
        
        misclassified = predictions != targets
        
        if np.sum(misclassified) == 0:
            return "No misclassifications found! Perfect performance!"
        
        analysis = []
        analysis.append("=== MISCLASSIFICATION ANALYSIS ===\n")
        
        # ì „ì²´ ì˜¤ë¶„ë¥˜ í†µê³„
        total_errors = np.sum(misclassified)
        total_samples = len(predictions)
        error_rate = total_errors / total_samples
        
        analysis.append(f"Total Misclassifications: {total_errors}/{total_samples}")
        analysis.append(f"Error Rate: {error_rate:.1%}\n")
        
        # í´ë˜ìŠ¤ë³„ ì˜¤ë¶„ë¥˜ ë§¤íŠ¸ë¦­ìŠ¤
        analysis.append("Class-wise Error Matrix:")
        for true_class in range(len(GESTURE_CLASSES)):
            true_name = GESTURE_CLASSES_EN[true_class]
            errors_for_class = misclassified & (targets == true_class)
            
            if np.sum(errors_for_class) > 0:
                analysis.append(f"\n{true_name} misclassified as:")
                pred_classes = predictions[errors_for_class]
                for pred_class in np.unique(pred_classes):
                    count = np.sum(pred_classes == pred_class)
                    pred_name = GESTURE_CLASSES_EN[pred_class]
                    analysis.append(f"  - {pred_name}: {count} times")
        
        # ì‹ ë¢°ë„ ë¶„ì„
        analysis.append(f"\nConfidence Analysis:")
        analysis.append(f"Average confidence (correct): {np.mean(confidences[~misclassified]):.3f}")
        analysis.append(f"Average confidence (incorrect): {np.mean(confidences[misclassified]):.3f}")
        
        # ê°€ì¥ í™•ì‹ í•˜ëŠ” ì˜¤ë¶„ë¥˜
        if np.sum(misclassified) > 0:
            high_conf_errors = misclassified & (confidences > 0.9)
            if np.sum(high_conf_errors) > 0:
                analysis.append(f"\nHigh-confidence errors (>0.9): {np.sum(high_conf_errors)}")
                analysis.append("This suggests potential systematic issues in the model.")
        
        return "\n".join(analysis)
    
    def save_evaluation_report(self, metrics, results, save_path=None):
        """í‰ê°€ ë³´ê³ ì„œ ì €ì¥"""
        if not save_path:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            save_path = self.results_dir / f"evaluation_report_{timestamp}.json"
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'model_path': str(self.model_path),
            'overall_metrics': {
                'accuracy': float(metrics['accuracy']),
                'precision': float(metrics['precision']),
                'recall': float(metrics['recall']),
                'f1_score': float(metrics['f1_score'])
            },
            'confidence_stats': metrics['confidence_stats'],
            'class_report': metrics['class_report']
        }
        
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"Evaluation report saved: {save_path}")
        return save_path
    
    def run_full_evaluation(self):
        """ì „ì²´ í‰ê°€ ì‹¤í–‰"""
        logger.info("=== TCN ëª¨ë¸ ì¢…í•© í‰ê°€ ì‹œì‘ ===")
        
        # 1. ëª¨ë¸ ë° ë°ì´í„° ë¡œë“œ
        if not self.load_model():
            return False
        
        if not self.load_test_data():
            return False
        
        # 2. ëª¨ë¸ í‰ê°€
        logger.info("ëª¨ë¸ í‰ê°€ ì¤‘...")
        results = self.evaluate_model()
        if not results:
            return False
        
        # 3. ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        logger.info("ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° ì¤‘...")
        metrics = self.calculate_metrics(results)
        
        # 4. ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*50)
        print("ğŸ“Š TCN ëª¨ë¸ í‰ê°€ ê²°ê³¼")
        print("="*50)
        print(f"âœ… ì „ì²´ ì •í™•ë„: {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)")
        print(f"ğŸ“ˆ ì •ë°€ë„: {metrics['precision']:.4f}")
        print(f"ğŸ“‰ ì¬í˜„ìœ¨: {metrics['recall']:.4f}")
        print(f"ğŸ¯ F1-Score: {metrics['f1_score']:.4f}")
        print(f"ğŸ” í‰ê·  ì‹ ë¢°ë„: {metrics['confidence_stats']['mean']:.4f}")
        print("="*50)
        
        # 5. ìƒì„¸ ë¶„ì„ ë° ì‹œê°í™”
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        logger.info("ìƒì„¸ ì‹œê°í™” ìƒì„± ì¤‘...")
        
        # Confusion Matrix
        cm_path = self.results_dir / f"confusion_matrix_{timestamp}.png"
        self.plot_confusion_matrix(results, cm_path)
        
        # ì‹ ë¢°ë„ ë¶„í¬
        conf_path = self.results_dir / f"confidence_distribution_{timestamp}.png"
        self.plot_confidence_distribution(results, conf_path)
        
        # ROC ê³¡ì„ 
        roc_path = self.results_dir / f"roc_curves_{timestamp}.png"
        self.plot_roc_curves(results, roc_path)
        
        # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¹„êµ
        class_perf_path = self.results_dir / f"class_performance_{timestamp}.png"
        self.plot_class_performance_comparison(results, class_perf_path)
        
        # ì˜¤ë¶„ë¥˜ ë¶„ì„
        error_path = self.results_dir / f"error_analysis_{timestamp}.png"
        self.plot_error_analysis(results, error_path)
        
        # ì„±ëŠ¥ ìš”ì•½
        summary_path = self.results_dir / f"performance_summary_{timestamp}.png"
        self.plot_performance_summary(metrics, summary_path)
        
        # 6. ì˜¤ë¶„ë¥˜ ìƒì„¸ ë¶„ì„
        logger.info("ì˜¤ë¶„ë¥˜ ë¶„ì„ ì¤‘...")
        misclass_analysis = self.analyze_misclassifications(results)
        
        # í…ìŠ¤íŠ¸ ë³´ê³ ì„œ ì €ì¥
        text_report_path = self.results_dir / f"misclassification_analysis_{timestamp}.txt"
        with open(text_report_path, 'w', encoding='utf-8') as f:
            f.write(misclass_analysis)
        logger.info(f"Misclassification analysis saved: {text_report_path}")
        
        # ì½˜ì†”ì— ìš”ì•½ ì¶œë ¥
        print("\n" + "="*60)
        print("ğŸ” MISCLASSIFICATION SUMMARY")
        print("="*60)
        lines = misclass_analysis.split('\n')
        for line in lines[:10]:  # ì²˜ìŒ 10ì¤„ë§Œ ì¶œë ¥
            print(line)
        if len(lines) > 10:
            print("... (ë” ìì„¸í•œ ë‚´ìš©ì€ íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”)")
        print("="*60)
        
        # 7. ì¢…í•© ë³´ê³ ì„œ ì €ì¥
        report_path = self.save_evaluation_report(metrics, results)
        
        logger.info("=== í‰ê°€ ì™„ë£Œ ===")
        return True

if __name__ == "__main__":
    evaluator = ModelEvaluator()
    evaluator.run_full_evaluation() 