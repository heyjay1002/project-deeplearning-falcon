#!/usr/bin/env python3
"""
BirdRiskSim YOLO Evaluation Script
í•™ìŠµëœ YOLOv8s ëª¨ë¸ì˜ ìƒì„¸í•œ ì„±ëŠ¥ í‰ê°€
"""

import os
import numpy as np
import pandas as pd
from pathlib import Path
import argparse
from ultralytics import YOLO
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
import cv2
from collections import defaultdict

class ModelEvaluator:
    def __init__(self, model_path="weights/best_bird_detection.pt", dataset_yaml="dataset.yaml"):
        """
        í‰ê°€ í´ë˜ìŠ¤ ì´ˆê¸°í™”
        
        Args:
            model_path: í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ
            dataset_yaml: ë°ì´í„°ì…‹ ì„¤ì • íŒŒì¼
        """
        self.model_path = model_path
        self.dataset_yaml = dataset_yaml
        self.model = None
        self.class_names = {0: 'Flock', 1: 'Airplane'}
        
        self.load_model()
    
    def load_model(self):
        """ëª¨ë¸ ë¡œë“œ"""
        if not Path(self.model_path).exists():
            print(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.model_path}")
            return False
        
        try:
            self.model = YOLO(self.model_path)
            print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.model_path}")
            return True
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def run_official_validation(self):
        """ê³µì‹ YOLO validation ì‹¤í–‰"""
        if self.model is None:
            print("âŒ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        print("ğŸ” ê³µì‹ YOLO Validation ì‹¤í–‰ ì¤‘...")
        
        try:
            metrics = self.model.val(
                data=self.dataset_yaml, 
                save_json=True, 
                plots=True,
                verbose=True
            )
            
            print("\nğŸ“Š ê³µì‹ í‰ê°€ ê²°ê³¼:")
            print(f"  mAP@0.5: {metrics.box.map50:.3f}")
            print(f"  mAP@0.5:0.95: {metrics.box.map:.3f}")
            print(f"  Precision: {metrics.box.mp:.3f}")
            print(f"  Recall: {metrics.box.mr:.3f}")
            print(f"  F1-Score: {metrics.box.f1:.3f}")
            
            # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥
            if hasattr(metrics.box, 'maps'):
                maps = metrics.box.maps
                print(f"\nğŸ“Š í´ë˜ìŠ¤ë³„ mAP@0.5:")
                for i, map_score in enumerate(maps):
                    if i < len(self.class_names):
                        print(f"  {self.class_names[i]}: {map_score:.3f}")
            
            return metrics
            
        except Exception as e:
            print(f"âŒ í‰ê°€ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return None
    
    def analyze_predictions_on_dataset(self, confidence_threshold=0.25):
        """ë°ì´í„°ì…‹ì— ëŒ€í•œ ì˜ˆì¸¡ ë¶„ì„"""
        print("ğŸ” ë°ì´í„°ì…‹ ì˜ˆì¸¡ ë¶„ì„ ì¤‘...")
        
        # ê²€ì¦ ë°ì´í„°ì…‹ ê²½ë¡œ
        val_images_dir = Path("dataset/images/val")
        val_labels_dir = Path("dataset/labels/val")
        
        if not val_images_dir.exists():
            print(f"âŒ ê²€ì¦ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {val_images_dir}")
            return
        
        image_files = list(val_images_dir.glob("*.png"))
        
        predictions = []
        ground_truths = []
        detection_stats = defaultdict(int)
        
        print(f"ğŸ“ {len(image_files)}ê°œ ê²€ì¦ ì´ë¯¸ì§€ ë¶„ì„ ì¤‘...")
        
        for i, img_file in enumerate(image_files):
            if (i + 1) % 50 == 0:
                print(f"  ì²˜ë¦¬ ì¤‘: {i+1}/{len(image_files)}")
            
            # Ground Truth ë¡œë“œ
            label_file = val_labels_dir / f"{img_file.stem}.txt"
            gt_boxes = []
            
            if label_file.exists() and label_file.stat().st_size > 0:
                with open(label_file, 'r') as f:
                    for line in f:
                        parts = line.strip().split()
                        if len(parts) >= 5:
                            class_id = int(parts[0])
                            gt_boxes.append(class_id)
            
            # ì˜ˆì¸¡ ìˆ˜í–‰
            results = self.model(str(img_file), conf=confidence_threshold, verbose=False)
            pred_boxes = []
            
            if results[0].boxes is not None:
                classes = results[0].boxes.cls.cpu().numpy().astype(int)
                confidences = results[0].boxes.conf.cpu().numpy()
                
                for cls, conf in zip(classes, confidences):
                    pred_boxes.append(cls)
                    detection_stats[f'{self.class_names[cls]}_detected'] += 1
            
            # í†µê³„ ìˆ˜ì§‘
            ground_truths.extend(gt_boxes)
            predictions.extend(pred_boxes)
            
            # ì´ë¯¸ì§€ë³„ í†µê³„
            detection_stats['total_images'] += 1
            if gt_boxes:
                detection_stats['images_with_gt'] += 1
            if pred_boxes:
                detection_stats['images_with_pred'] += 1
        
        # ê²°ê³¼ ë¶„ì„
        self.analyze_detection_results(ground_truths, predictions, detection_stats)
    
    def analyze_detection_results(self, ground_truths, predictions, stats):
        """íƒì§€ ê²°ê³¼ ìƒì„¸ ë¶„ì„"""
        print("\nğŸ“Š ìƒì„¸ ë¶„ì„ ê²°ê³¼:")
        print("=" * 50)
        
        # ê¸°ë³¸ í†µê³„
        print(f"ì´ ì´ë¯¸ì§€: {stats['total_images']}")
        print(f"GTê°€ ìˆëŠ” ì´ë¯¸ì§€: {stats['images_with_gt']}")
        print(f"ì˜ˆì¸¡ì´ ìˆëŠ” ì´ë¯¸ì§€: {stats['images_with_pred']}")
        
        # í´ë˜ìŠ¤ë³„ íƒì§€ ìˆ˜
        print(f"\ní´ë˜ìŠ¤ë³„ íƒì§€ ìˆ˜:")
        for cls_id, cls_name in self.class_names.items():
            key = f'{cls_name}_detected'
            count = stats.get(key, 0)
            print(f"  {cls_name}: {count}ê°œ")
        
        # Ground Truth ë¶„í¬
        if ground_truths:
            gt_counts = np.bincount(ground_truths)
            print(f"\nGround Truth ë¶„í¬:")
            for cls_id, count in enumerate(gt_counts):
                if cls_id in self.class_names:
                    print(f"  {self.class_names[cls_id]}: {count}ê°œ")
        
        # ì˜ˆì¸¡ ë¶„í¬
        if predictions:
            pred_counts = np.bincount(predictions)
            print(f"\nì˜ˆì¸¡ ë¶„í¬:")
            for cls_id, count in enumerate(pred_counts):
                if cls_id in self.class_names:
                    print(f"  {self.class_names[cls_id]}: {count}ê°œ")
        
        # í˜¼ë™ í–‰ë ¬ (í´ë˜ìŠ¤ ìˆ˜ì¤€ì—ì„œë§Œ, ë°”ìš´ë”© ë°•ìŠ¤ ë§¤ì¹­ì€ ë³µì¡í•¨)
        if ground_truths and predictions:
            self.plot_class_distribution(ground_truths, predictions)
    
    def plot_class_distribution(self, ground_truths, predictions):
        """í´ë˜ìŠ¤ ë¶„í¬ ì‹œê°í™”"""
        # í´ë˜ìŠ¤ ë¶„í¬ ë¹„êµ
        plt.figure(figsize=(15, 5))
        
        # Ground Truth ë¶„í¬
        plt.subplot(1, 3, 1)
        gt_counts = np.bincount(ground_truths)
        classes = [self.class_names[i] for i in range(len(gt_counts)) if i in self.class_names]
        counts = [gt_counts[i] for i in range(len(gt_counts)) if i in self.class_names]
        
        plt.bar(classes, counts, color=['green', 'red'][:len(classes)])
        plt.title('Ground Truth ë¶„í¬')
        plt.ylabel('ê°œìˆ˜')
        for i, v in enumerate(counts):
            plt.text(i, v + max(counts)*0.01, str(v), ha='center', va='bottom')
        
        # ì˜ˆì¸¡ ë¶„í¬
        plt.subplot(1, 3, 2)
        pred_counts = np.bincount(predictions)
        pred_counts_list = [pred_counts[i] if i < len(pred_counts) else 0 
                           for i in range(len(self.class_names)) 
                           if i in self.class_names]
        
        plt.bar(classes, pred_counts_list, color=['lightgreen', 'lightcoral'][:len(classes)])
        plt.title('ì˜ˆì¸¡ ë¶„í¬')
        plt.ylabel('ê°œìˆ˜')
        for i, v in enumerate(pred_counts_list):
            plt.text(i, v + max(pred_counts_list)*0.01, str(v), ha='center', va='bottom')
        
        # ë¹„êµ
        plt.subplot(1, 3, 3)
        x = np.arange(len(classes))
        width = 0.35
        
        plt.bar(x - width/2, counts, width, label='Ground Truth', color=['green', 'red'][:len(classes)], alpha=0.7)
        plt.bar(x + width/2, pred_counts_list, width, label='Predictions', color=['lightgreen', 'lightcoral'][:len(classes)], alpha=0.7)
        
        plt.title('GT vs ì˜ˆì¸¡ ë¹„êµ')
        plt.ylabel('ê°œìˆ˜')
        plt.xticks(x, classes)
        plt.legend()
        
        plt.tight_layout()
        
        # ê²°ê³¼ ì €ì¥
        output_dir = Path("training/results/evaluation")
        output_dir.mkdir(exist_ok=True)
        
        # í´ë˜ìŠ¤ ë¶„í¬ ê·¸ë˜í”„ ì €ì¥
        plt.savefig(output_dir / 'class_distribution.png', dpi=300, bbox_inches='tight')
        print(f"âœ… í´ë˜ìŠ¤ ë¶„í¬ ê·¸ë˜í”„ ì €ì¥: {output_dir}/class_distribution.png")
    
    def confidence_analysis(self, confidence_thresholds=[0.1, 0.25, 0.5, 0.75, 0.9]):
        """ì‹ ë¢°ë„ ì„ê³„ê°’ë³„ ì„±ëŠ¥ ë¶„ì„"""
        print("ğŸ” ì‹ ë¢°ë„ ì„ê³„ê°’ë³„ ì„±ëŠ¥ ë¶„ì„...")
        
        val_images_dir = Path("dataset/images/val")
        image_files = list(val_images_dir.glob("*.png"))[:100]  # ìƒ˜í”Œë§
        
        results = []
        
        for conf_thresh in confidence_thresholds:
            total_detections = 0
            class_counts = {0: 0, 1: 0}
            
            for img_file in image_files:
                preds = self.model(str(img_file), conf=conf_thresh, verbose=False)
                
                if preds[0].boxes is not None:
                    classes = preds[0].boxes.cls.cpu().numpy().astype(int)
                    total_detections += len(classes)
                    
                    for cls in classes:
                        class_counts[cls] += 1
            
            results.append({
                'confidence': conf_thresh,
                'total_detections': total_detections,
                'flock_detections': class_counts[0],
                'airplane_detections': class_counts[1]
            })
        
        # ê²°ê³¼ ì‹œê°í™”
        df = pd.DataFrame(results)
        
        plt.figure(figsize=(12, 4))
        
        plt.subplot(1, 2, 1)
        plt.plot(df['confidence'], df['total_detections'], 'o-', label='Total')
        plt.plot(df['confidence'], df['flock_detections'], 's-', label='Flock')
        plt.plot(df['confidence'], df['airplane_detections'], '^-', label='Airplane')
        plt.xlabel('Confidence Threshold')
        plt.ylabel('Detection Count')
        plt.title('ì‹ ë¢°ë„ë³„ íƒì§€ ìˆ˜')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.subplot(1, 2, 2)
        flock_ratio = df['flock_detections'] / (df['flock_detections'] + df['airplane_detections'] + 1e-6)
        airplane_ratio = df['airplane_detections'] / (df['flock_detections'] + df['airplane_detections'] + 1e-6)
        
        plt.plot(df['confidence'], flock_ratio, 's-', label='Flock %', color='green')
        plt.plot(df['confidence'], airplane_ratio, '^-', label='Airplane %', color='red')
        plt.xlabel('Confidence Threshold')
        plt.ylabel('Class Ratio')
        plt.title('ì‹ ë¢°ë„ë³„ í´ë˜ìŠ¤ ë¹„ìœ¨')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ì‹ ë¢°ë„ ë¶„ì„ ê·¸ë˜í”„ ì €ì¥
        plt.savefig(output_dir / 'confidence_analysis.png', dpi=300, bbox_inches='tight')
        print(f"âœ… ì‹ ë¢°ë„ ë¶„ì„ ì €ì¥: {output_dir}/confidence_analysis.png")
        
        # ê²°ê³¼ í‘œ ì¶œë ¥
        print(f"\nğŸ“Š ì‹ ë¢°ë„ë³„ íƒì§€ ê²°ê³¼:")
        print(df.to_string(index=False))

def main():
    parser = argparse.ArgumentParser(description='BirdRiskSim YOLO Evaluation')
    parser.add_argument('--model', type=str, default='weights/best_bird_detection.pt',
                       help='Model path')
    parser.add_argument('--data', type=str, default='dataset.yaml',
                       help='Dataset YAML path')
    parser.add_argument('--conf', type=float, default=0.25,
                       help='Confidence threshold for analysis')
    parser.add_argument('--skip-official', action='store_true',
                       help='Skip official YOLO validation')
    
    args = parser.parse_args()
    
    print("ğŸš€ BirdRiskSim YOLO ëª¨ë¸ í‰ê°€ ì‹œì‘")
    print("=" * 60)
    
    # í‰ê°€ ê°ì²´ ìƒì„±
    evaluator = ModelEvaluator(args.model, args.data)
    
    if evaluator.model is None:
        return
    
    # ê²°ê³¼ í´ë” ìƒì„±
    output_dir = Path("training/results/evaluation")
    output_dir.mkdir(exist_ok=True)
    
    # 1. ê³µì‹ ê²€ì¦
    if not args.skip_official:
        print("\n1ï¸âƒ£ ê³µì‹ YOLO Validation...")
        metrics = evaluator.run_official_validation()
    
    # 2. ì˜ˆì¸¡ ë¶„ì„
    print("\n2ï¸âƒ£ ë°ì´í„°ì…‹ ì˜ˆì¸¡ ë¶„ì„...")
    evaluator.analyze_predictions_on_dataset(args.conf)
    
    # 3. ì‹ ë¢°ë„ ë¶„ì„
    print("\n3ï¸âƒ£ ì‹ ë¢°ë„ ì„ê³„ê°’ ë¶„ì„...")
    evaluator.confidence_analysis()
    
    print(f"\nğŸ‰ í‰ê°€ ì™„ë£Œ!")
    print(f"ğŸ“ ê²°ê³¼ í´ë”: {output_dir}/")
    print("=" * 60)

if __name__ == "__main__":
    main() 